{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-870210b50e3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmplleaflet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_columns'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mplleaflet\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m from mplleaflet._display import (\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mshow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\mplleaflet\\_display.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmplexporter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexporter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExporter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPackageLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mleaflet_renderer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLeafletRenderer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jinja2\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# high level interface\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTemplate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# loaders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\jinja2\\environment.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEvalContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCodeGenerator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mruntime\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUndefined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjinja2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTemplateSyntaxError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTemplateNotFound\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from scipy.integrate import quad\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import time\n",
    "import mplleaflet\n",
    "pd.set_option('display.max_columns',60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files_2017 = [('2017' + \"%.2d\" + '-citibike-tripdata.csv') % i for i in range(1, 13)]\n",
    "csv_files_2018 = [('2018' + \"%.2d\" + '-citibike-tripdata.csv') % i for i in range(1, 13)]\n",
    "csv_files_2019 = [('2019' + \"%.2d\" + '-citibike-tripdata.csv') % i for i in range(1, 13)]\n",
    "csv_files = csv_files_2017 + csv_files_2018 + csv_files_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_datetime(df):\n",
    "    df1 = df.copy()\n",
    "    df1['starttime'] = pd.to_datetime(df1['starttime'])\n",
    "    df1['stoptime'] = pd.to_datetime(df1['stoptime'])\n",
    "    df1['start_date'] = df1['starttime'].dt.date\n",
    "    df1['start_time'] = df1['starttime'].dt.time\n",
    "    df1['start_hour'] = df1['starttime'].dt.hour\n",
    "    df1['start_min'] = df1['starttime'].dt.minute\n",
    "    df1['start_year'] = df1['starttime'].dt.year\n",
    "    df1['start_month'] = df1['starttime'].dt.month\n",
    "    df1['start_dayofweek'] = df1['starttime'].dt.weekday   # Monday is 0, Sunday is 6\n",
    "    #df1['start_dayofweek'] = df1['starttime'].dt.weekday_name  # The name of day in a week (e.g. Monday)\n",
    "    df1['stop_date'] = df1['stoptime'].dt.date\n",
    "    df1['stop_time'] = df1['stoptime'].dt.time\n",
    "    df1['stop_hour'] = df1['stoptime'].dt.hour\n",
    "    df1['stop_min'] = df1['stoptime'].dt.minute\n",
    "    df1['stop_dayofweek'] = df1['stoptime'].dt.weekday\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_offpeak(df):\n",
    "    df1 = df.copy()\n",
    "    mask = df1['start_hour'].apply(lambda x: 5 <= x <= 22)\n",
    "    return df1[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lonlat2mile( lon_start,lat_start,lon_end, lat_end):\n",
    "#     delta_y = (lat_end - lat_start) * 69\n",
    "# #     def integrand( lat ):\n",
    "# #         return np.cos( np.pi * lat / 180 ) * 69.172\n",
    "# #     delta_x, xerr = quad(integrand, lat_start, lat_end )\n",
    "#     delta_x = (lon_end - lon_start)*53\n",
    "#     return np.abs( delta_x ) + np.abs( delta_y )\n",
    "# # lonlat2mile(-73.983035,40.744449,-73.948813,40.778301)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregated_data(df):\n",
    "    # Define distance: if same start and end location, use average speed of 7.456mph to estimate distance.\n",
    "    # If different locations, calculate Manhattan distance between two stations\n",
    "    df['distance']=np.where(df['start_station_ID'] == df['end_station_ID'],df['trip_duration']*7.456/3600,\n",
    "                            abs(df['start_station_longitude']-df['end_station_longitude'])*53+\\\n",
    "                            abs(df['start_station_latitude']-df['end_station_latitude'])*69)\n",
    "    # Daily average of all stations for time-series analysis\n",
    "    df_daily = df.groupby('start_date').agg({'trip_duration':['count','mean'],'distance':'mean'}).reset_index()\n",
    "    df_daily.columns = ['start_date','trip_per_day','daily_avg_trip_duration','daily_avg_distance']\n",
    "    df_daily_merged = df.merge(df_daily, how = 'left', on = 'start_date')\n",
    "    # Hourly average for each station regardeless of days \n",
    "    df_hourly = df.groupby(['start_station_ID','start_hour']).\\\n",
    "    agg({'trip_duration':['count','mean'],'distance':'mean'}).reset_index()\n",
    "    df_hourly.columns = ['start_station_ID','start_hour','trip_per_hour','hourly_avg_trip_duration','hourly_avg_distance']\n",
    "    df_hourly_merged = df_daily_merged.merge(df_hourly, how = 'left', on = ['start_station_ID','start_hour'])\n",
    "    # Calculate hourly trip counts, avg trip duration, and avg trip distance per station and merge to above df\n",
    "    df_hourly_eachday = df.groupby(['start_station_ID','start_date','start_hour']).\\\n",
    "    agg({'trip_duration':['count','mean'],'distance':'mean'}).reset_index()\n",
    "    df_hourly_eachday.columns = ['start_station_ID','start_date','start_hour','trip_per_hour_eachday','hourly_avg_trip_duration_eachday','hourly_avg_distance_eachday']\n",
    "    df_hourly_eachday_merged = df_hourly_merged.merge(df_hourly_eachday, how = 'left', on = ['start_station_ID','start_date','start_hour'])\n",
    "    return df_hourly_eachday_merged    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bikecount(df):\n",
    "    # groupby start station ID, date and hour to get hourly counts of trips per start station\n",
    "    checkout = df.groupby(['start_station_ID','start_date','start_hour'])['trip_duration'].count().reset_index()\n",
    "    checkout.columns = ['start_station_ID','start_date','start_hour','checkout_counts']\n",
    "    # groupby end station ID, date, and hour to get hourly counts of trips per end station \n",
    "    checkin = df.groupby(['end_station_ID','stop_date','stop_hour'])[['trip_duration']].count().reset_index()\n",
    "    checkin.columns=['end_station_ID','stop_date','stop_hour','checkin_counts']\n",
    "    # Join dataframe to get station checkin and checkout counts \n",
    "    temp = pd.merge(checkout, checkin,  how='outer', left_on=['start_station_ID','start_date','start_hour'], \n",
    "                    right_on = ['end_station_ID','stop_date','stop_hour'])\n",
    "    temp['start_station_ID'] = temp['start_station_ID'].fillna(temp['end_station_ID'])\n",
    "    temp['start_date'] = temp['start_date'].fillna(temp['stop_date'])\n",
    "    temp['start_hour'] = temp['start_hour'].fillna(temp['stop_hour'])\n",
    "    temp['checkout_counts'] = temp['checkout_counts'].fillna(0)\n",
    "    temp['checkin_counts'] = temp['checkin_counts'].fillna(0)\n",
    "    temp = temp.drop(['end_station_ID','stop_date','stop_hour'],axis=1)\n",
    "    temp.columns=['station_ID','date','hour','checkout_counts','checkin_counts']\n",
    "    temp['bike_added'] = temp['checkin_counts'] - temp['checkout_counts']\n",
    "    # merge orginal dataframe to get hourly checkin/checkout information for both start and stop stations \n",
    "    df_temp_merged = pd.merge(df, temp,  how='left', left_on=['start_station_ID','start_date','start_hour'], \n",
    "         right_on = ['station_ID','date','hour']).drop(['station_ID','date','hour'],axis = 1)\n",
    "    df_temp_merged = pd.merge(df_temp_merged, temp,  how='left', left_on=['end_station_ID','stop_date','stop_hour'], \n",
    "         right_on = ['station_ID','date','hour']).drop(['station_ID','date','hour'],axis = 1) \n",
    "    df_temp_merged = df_temp_merged.rename(columns={'checkout_counts_x':'start_station_checkout_counts',\n",
    "                                                    'checkin_counts_x':'start_station_checkin_counts',\n",
    "                                                    'bike_added_x':'start_station_bike_added', \n",
    "                                                    'checkout_counts_y':'end_station_checkout_counts',\n",
    "                                                    'checkin_counts_y':'end_station_checkin_counts',\n",
    "                                                    'bike_added_y':'end_station_bike_added'}) \n",
    "    return df_temp_merged\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "random.seed(0)\n",
    "path = '../Tripdata/'\n",
    "for i, csv in enumerate(csv_files):\n",
    "    start_time = time.time()\n",
    "    df_temp = pd.read_csv(path+csv)\n",
    "    df_temp.columns = ['trip_duration','starttime','stoptime','start_station_ID','start_station_name',\n",
    "                       'start_station_latitude','start_station_longitude','end_station_ID','end_station_name',\n",
    "                       'end_station_latitude','end_station_longitude','bike_ID','user_type','birth_year','gender']\n",
    "    df_temp = df_temp.loc[df_temp['trip_duration']<= 24*3600] # remove trips that are longer than 1 day \n",
    "    df_temp = df_temp.loc[(df_temp['start_station_latitude']>40) & (df_temp['start_station_latitude']<41)] # remove areas that are not in NYC (equator and montreal)\n",
    "    df_temp = to_datetime(df_temp)\n",
    "    df_temp = ignore_offpeak(df_temp)\n",
    "    df_temp = merge_bikecount(df_temp)\n",
    "    df_temp = aggregated_data(df_temp)\n",
    "    \n",
    "    # take a 5% subset of monthly file for analysis and another 5% of the remaining dataset as test dataset for ML\n",
    "    rows = len(df_temp)\n",
    "    size = int(rows/20)\n",
    "    selected_idx = random.sample(range(1,rows), size)\n",
    "    skip_idx = list(set(df_temp.index)-set(selected_idx))\n",
    "    test_idx = random.sample(skip_idx,int(len(skip_idx)/20))\n",
    "    df_train = df_temp.iloc[selected_idx,:]\n",
    "    df_test = df_temp.iloc[test_idx,:]\n",
    "    \n",
    "    # save train and test datset \n",
    "    df_train.to_csv(months[i%12] + csv[:4] + 'train.csv')\n",
    "    df_test.to_csv(months[i%12] + csv[:4] + 'test.csv')\n",
    "    print('Finishing data extraction from ' + csv)\n",
    "    timeSpent = time.time() - start_time\n",
    "    print('This iteration uses %.2f'%(timeSpent))\n",
    "    total_time += timeSpent\n",
    "    print(total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = []\n",
    "test_list = []\n",
    "for i, csv in enumerate(csv_files):\n",
    "    train_list.append(months[i%12] + csv[:4] + 'train.csv')\n",
    "    test_list.append(months[i%12] + csv[:4] + 'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all months train.csv into one train dataframe\n",
    "train_df = pd.DataFrame()\n",
    "for i in range(len(train_list)):\n",
    "    temp_df = pd.read_csv(train_list[i], index_col = 0)\n",
    "    train_df = pd.concat([train_df, temp_df], axis = 0)\n",
    "    print('Finished ' + str(i) + ' element')\n",
    "train_df.to_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge all months test.csv into one test dataframe\n",
    "test_df = pd.DataFrame()\n",
    "for i in range(len(test_list)):\n",
    "    temp_df = pd.read_csv(test_list[i], index_col = 0)\n",
    "    test_df = pd.concat([test_df, temp_df], axis = 0)\n",
    "    print('Finished ' + str(i) + ' element')\n",
    "test_df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.read_csv('weather.csv',index_col = 0).reset_index()\n",
    "weather = weather.fillna(0)\n",
    "weather['DATE'] = pd.to_datetime(weather['DATE'])\n",
    "weather['TAVG'] = (weather['TMIN']+weather['TMAX'])/2\n",
    "weather['HasPRCP'] = [1 if x !=0 else 0 for x in weather['PRCP']]\n",
    "weather['HasSNOW'] = [1 if x !=0 else 0 for x in weather['SNOW']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['start_date'] = pd.to_datetime(train_df['start_date'])\n",
    "train_df_weather = train_df.merge(weather,how='left',\n",
    "                                  left_on = 'start_date', right_on = 'DATE').drop(['DATE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['start_date'] = pd.to_datetime(test_df['start_date'])\n",
    "test_df_weather = test_df.merge(weather,how='left',\n",
    "                                  left_on = 'start_date', right_on = 'DATE').drop(['DATE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather.to_csv('train_weather.csv')\n",
    "test_df_weather.to_csv('test_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Merged Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather = pd.read_csv('train_weather.csv',index_col = 0)\n",
    "test_df_weather = pd.read_csv('test_weather.csv',index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather['Isweekday'] = [0 if (x ==5 or x==6) else 1 for x in train_df_weather['start_dayofweek'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming bikers during peak hours on weekdays are commuting\n",
    "rushhours = [8,9,16,17,18,19]\n",
    "train_df_weather['Commute'] = np.where(((train_df_weather['Isweekday']==1) &\n",
    "                                        (train_df_weather['start_hour'].isin(rushhours))),1,0)\n",
    "train_df_weather[['Isweekday','start_hour','Commute']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'Commute',data = train_df_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Weekday and Weekend Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weekday = train_df_weather[['Isweekday','trip_per_day','daily_avg_trip_duration','daily_avg_distance']]\n",
    "Weekday.columns =  ['Isweekday','Daily Trip Count','Avg Trip Duration','Avg Distance']\n",
    "Weekday = Weekday.groupby('Isweekday').agg('mean').reset_index()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "cols_plot = ['Daily Trip Count','Avg Trip Duration','Avg Distance']\n",
    "axes = Weekday[cols_plot].plot(kind='bar', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True,rot = 0)\n",
    "axes[0].set_ylabel('Daily Trip Count')\n",
    "axes[1].set_ylabel('Avg Trip Duration(s)')\n",
    "axes[2].set_ylabel('Avg Distance(mile)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Rush Hours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the start_hour and see the frequency correspond to each starting hour\n",
    "train_df_weather.groupby('start_hour').count()[['trip_duration']].sort_values(by = 'trip_duration', ascending = False).head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 stations during rush hours on weekdays\n",
    "top50_weekdays = train_df_weather.loc[train_df_weather['Isweekday']==1].groupby(['start_station_name']).agg({'trip_duration':'count','start_station_latitude':lambda x: x.iloc[0], 'start_station_longitude':lambda x: x.iloc[0]}).\\\n",
    "sort_values(by = 'trip_duration', ascending = False).head(50)\n",
    "top50_weekdays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 stations on weekends\n",
    "top50_weekdays_rush_start = train_df_weather.loc[train_df_weather['Commute']==1].groupby(['start_station_name']).agg({'trip_duration':'mean','start_station_latitude':lambda x: x.iloc[0], 'start_station_longitude':lambda x: x.iloc[0]}).\\\n",
    "sort_values(by = 'trip_duration', ascending = False).head(50)\n",
    "top50_weekdays_rush_start.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 50 stations on weekends\n",
    "weekends = train_df_weather.loc[train_df_weather['Isweekday']==0].groupby(['start_station_name']).agg({'trip_per_hour':'mean','start_station_latitude':lambda x: x.iloc[0], 'start_station_longitude':lambda x: x.iloc[0]}).\\\n",
    "sort_values(by = 'trip_per_hour', ascending = False).tail(50)\n",
    "weekends.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map for dock stations during weekdays and weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(top50_weekdays['start_station_longitude'].values, top50_weekdays['start_station_latitude'].values, 'ro',alpha = 0.5,markersize=8)\n",
    "plt.plot(top50_weekdays_rush_start['start_station_longitude'].values, top50_weekdays_rush_start['start_station_latitude'].values, 'bv',alpha = 0.5,markersize=8)\n",
    "#plt.plot(top50_weekends['start_station_longitude'].values, top50_weekends['start_station_latitude'].values, 'bv',alpha = 0.5,markersize=8)\n",
    "mplleaflet.display(tiles='cartodb_positron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of stations in different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of stations in different years\n",
    "train_df_weather.loc[train_df_weather.start_year == 2019].start_station_ID.nunique() \n",
    "train_df_weather.start_station_ID.nunique() \n",
    "# 2017 has 799, 2018 has 812, 2019 has 926, unique stations\n",
    "# total of 1011 stations \n",
    "station_2017 = list(train_df_weather.loc[train_df_weather.start_year == 2017].start_station_ID.unique())\n",
    "station_2018 = list(train_df_weather.loc[train_df_weather.start_year == 2018].start_station_ID.unique())\n",
    "station_2019 = list(train_df_weather.loc[train_df_weather.start_year == 2019].start_station_ID.unique())\n",
    "remove2018 =[item for item in station_2017 if item not in station_2018]\n",
    "new2018 =[item for item in station_2018 if item not in station_2017]\n",
    "remove2019 = [item for item in station_2018 if item not in station_2019]\n",
    "new2019 =[item for item in station_2019 if item not in station_2018]\n",
    "print('Number of stations removed in 2018: %.f' %len(remove2018))\n",
    "print('Number of stations removed in 2019: %.f' %len(remove2019))\n",
    "print('Number of stations added in 2018: %.f' %len(new2018))\n",
    "print('Number of stations added in 2019: %.f' %len(new2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal Trend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series of Trip Count\n",
    "daily_trip = train_df_weather[['start_date','trip_per_day','daily_avg_trip_duration','daily_avg_distance']].sort_values(by='start_date')\n",
    "daily_trip.columns = ['Date','Daily Trip Count','Avg Trip Duration','Avg Distance']\n",
    "daily_trip = daily_trip.groupby('Date').agg('mean').reset_index().set_index('Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp= pd.read_csv('../Tripdata/201710-citibike-tripdata.csv')\n",
    "df_temp.columns = ['trip_duration','starttime','stoptime','start_station_ID','start_station_name',\n",
    "                       'start_station_latitude','start_station_longitude','end_station_ID','end_station_name',\n",
    "                       'end_station_latitude','end_station_longitude','bike_ID','user_type','birth_year','gender']\n",
    "df_temp = df_temp.loc[df_temp['trip_duration']<= 24*3600] # remove trips that are longer than 1 day \n",
    "df_temp = to_datetime(df_temp)\n",
    "df_temp = ignore_offpeak(df_temp)\n",
    "df_temp = aggregated_data(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.loc[df_temp['distance']>4].sort_values(by='distance',ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11, 4)})\n",
    "cols_plot = ['Daily Trip Count','Avg Trip Duration','Avg Distance']\n",
    "axes = daily_trip[cols_plot].plot(marker='.', alpha=0.5, linestyle='None', figsize=(11, 9), subplots=True)\n",
    "axes[0].set_ylabel('Daily Trip Count')\n",
    "axes[1].set_ylabel('Avg Trip Duration(s)')\n",
    "axes[2].set_ylabel('Avg Distance(mile)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip counts by month\n",
    "month_trip = train_df_weather[['start_month','trip_per_day','daily_avg_trip_duration','daily_avg_distance']].sort_values(by='start_month')\n",
    "month_trip.columns=['Month','Avg Trip Counts','Avg Trip Duration','Avg Distance'] \n",
    "sns.boxplot(x=\"Month\", y=\"Avg Trip Counts\", data=month_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip Duration by Month\n",
    "sns.boxplot(x=\"Month\", y=\"Avg Trip Duration\",  data=month_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip distance by Month\n",
    "sns.boxplot(x=\"Month\", y=\"Avg Distance\", data=month_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(train_df.starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non typical checkout (Consider removing them )\n",
    "train_df_weather.loc[train_df_weather.distance > 10].sort_values(by = 'trip_duration', ascending = False).\\\n",
    "head(20)[['trip_duration', 'starttime', 'stoptime', 'user_type', 'distance']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather.loc[(train_df_weather.distance > 10) & (train_df_weather.user_type == 'Subscriber')].shape[0]/\\\n",
    "train_df_weather.loc[(train_df_weather.distance > 10)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "14306/2479389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather.loc[train_df_weather.user_type == 'Subscriber'].shape[0]/train_df_weather.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_weather.loc[train_df_weather.user_type != 'Subscriber'].shape[0]/train_df_weather.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_df_weather.groupby('start_month').mean()[['trip_duration']].reset_index()\n",
    "sns.barplot(x = a.start_month, y = a.trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = train_df_weather.groupby('start_month').count()[['trip_duration']].reset_index()\n",
    "sns.barplot(x = b.start_month, y = b.trip_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = train_df_weather.groupby('start_month').mean()[['distance']].reset_index()\n",
    "sns.barplot(x = c.start_month, y = c.distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature's impact on trip counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Temperature\n",
    "TEMP = train_df_weather[['TAVG','trip_duration','trip_per_day','daily_avg_distance']]\n",
    "TEMP.columns = ['Avg Temp','Avg Trip Duration(s)','Trip Count per Day','Avg Distance(mile)']\n",
    "sns.scatterplot(x='Avg Temp',y='Trip Count per Day',data = TEMP).set(title = 'Trip Count per Day v.s. Average Temperature')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unusual Weather Condition's impact on Bike Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extreme_weather = train_df_weather[['HasPRCP','HasSNOW','Fog', 'Heavy_Fog', 'Thunder', 'Haze']].apply(pd.value_counts)\n",
    "extreme_weather= extreme_weather.unstack().reset_index()\n",
    "extreme_weather.columns=['Weather Condition','Yes/No','Trip Counts']\n",
    "extreme_weather['Yes/No']=['Yes' if x==1 else 'No' for x in extreme_weather['Yes/No']]\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.rcParams[\"axes.labelsize\"] = 20\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title(\"Weather Condition's Impact on Trip Counts\", fontdict = {'fontsize' : 25})\n",
    "sns.barplot(x='Weather Condition', y ='Trip Counts', hue ='Yes/No', data = extreme_weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Heat Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns \n",
    "\n",
    "sns.set(style=\"white\")\n",
    "corr =train_df_weather.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.to_csv('Correlation_Matrix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
